{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# GridSearch with Census Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook shows how to use Fairlearn and the Fairness dashboard to generate predictors\nfor the Census dataset.\nThis dataset is a classification problem - given a range of data about 32,000 individuals,\npredict whether their annual income is above or below fifty thousand dollars per year.\n\nFor the purposes of this notebook, we shall treat this as a loan decision problem.\nWe will pretend that the label indicates whether or not each individual repaid a loan in\nthe past.\nWe will use the data to train a predictor to predict whether previously unseen individuals\nwill repay a loan or not.\nThe assumption is that the model predictions are used to decide whether an individual\nshould be offered a loan.\n\nWe will first train a fairness-unaware predictor and show that it leads to unfair\ndecisions under a specific notion of fairness called *demographic parity*.\nWe then mitigate unfairness by applying the :code:`GridSearch` algorithm from the\nFairlearn package.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and preprocess the data set\nWe download the data set using `fetch_adult` function in `fairlearn.datasets`.\nWe start by importing the various modules we're going to use:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from fairlearn.widget import FairlearnDashboard\nfrom sklearn.model_selection import train_test_split\nfrom fairlearn.reductions import GridSearch\nfrom fairlearn.reductions import DemographicParity, ErrorRate\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nimport pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now load and inspect the data by using the `fairlearn.datasets` module:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\ndata = fetch_openml(data_id=1590, as_frame=True)\nX_raw = data.data\nY = (data.target == '>50K') * 1\nX_raw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to treat the sex of each individual as a sensitive\nfeature (where 0 indicates female and 1 indicates male), and in\nthis particular case we are going separate this feature out and drop it\nfrom the main data.\nWe then perform some standard data preprocessing steps to convert the\ndata into a format suitable for the ML algorithms\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "A = X_raw[\"sex\"]\nX = X_raw.drop(labels=['sex'], axis=1)\nX = pd.get_dummies(X)\n\nsc = StandardScaler()\nX_scaled = sc.fit_transform(X)\nX_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n\nle = LabelEncoder()\nY = le.fit_transform(Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we split the data into training and test sets:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test, A_train, A_test = train_test_split(X_scaled,\n                                                                     Y,\n                                                                     A,\n                                                                     test_size=0.2,\n                                                                     random_state=0,\n                                                                     stratify=Y)\n\n# Work around indexing bug\nX_train = X_train.reset_index(drop=True)\nA_train = A_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\nA_test = A_test.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training a fairness-unaware predictor\n\nTo show the effect of Fairlearn we will first train a standard ML predictor\nthat does not incorporate fairness.\nFor speed of demonstration, we use the simple\n:class:`sklearn.linear_models.LogisticRegression` class:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "unmitigated_predictor = LogisticRegression(solver='liblinear', fit_intercept=True)\n\nunmitigated_predictor.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can load this predictor into the Fairness dashboard, and assess its fairness:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "FairlearnDashboard(sensitive_features=A_test, sensitive_feature_names=['sex'],\n                   y_true=Y_test,\n                   y_pred={\"unmitigated\": unmitigated_predictor.predict(X_test)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the disparity in accuracy, we see that males have an error\nabout three times greater than the females.\nMore interesting is the disparity in opportunity - males are offered loans at\nthree times the rate of females.\n\nDespite the fact that we removed the feature from the training data, our\npredictor still discriminates based on sex.\nThis demonstrates that simply ignoring a sensitive feature when fitting a\npredictor rarely eliminates unfairness.\nThere will generally be enough other features correlated with the removed\nfeature to lead to disparate impact.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mitigation with GridSearch\n\nThe :class:`fairlearn.reductions.GridSearch` class implements a simplified version of the\nexponentiated gradient reduction of `Agarwal et al. 2018 <https://arxiv.org/abs/1803.02453>`_.\nThe user supplies a standard ML estimator, which is treated as a blackbox.\n`GridSearch` works by generating a sequence of relabellings and reweightings, and\ntrains a predictor for each.\n\nFor this example, we specify demographic parity (on the sensitive feature of sex) as\nthe fairness metric.\nDemographic parity requires that individuals are offered the opportunity (are approved\nfor a loan in this example) independent of membership in the sensitive class (i.e., females\nand males should be offered loans at the same rate).\nWe are using this metric for the sake of simplicity; in general, the appropriate fairness\nmetric will not be obvious.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sweep = GridSearch(LogisticRegression(solver='liblinear', fit_intercept=True),\n                   constraints=DemographicParity(),\n                   grid_size=71)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our algorithms provide :code:`fit()` and :code:`predict()` methods, so they behave in a similar manner\nto other ML packages in Python.\nWe do however have to specify two extra arguments to :code:`fit()` - the column of sensitive\nfeature labels, and also the number of predictors to generate in our sweep.\n\nAfter :code:`fit()` completes, we extract the full set of predictors from the\n:class:`fairlearn.reductions.GridSearch` object.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sweep.fit(X_train, Y_train,\n          sensitive_features=A_train)\n\npredictors = sweep.predictors_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We could load these predictors into the Fairness dashboard now.\nHowever, the plot would be somewhat confusing due to their number.\nIn this case, we are going to remove the predictors which are dominated in the\nerror-disparity space by others from the sweep (note that the disparity will only be\ncalculated for the sensitive feature; other potentially sensitive features will\nnot be mitigated).\nIn general, one might not want to do this, since there may be other considerations\nbeyond the strict optimization of error and disparity (of the given sensitive feature).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "errors, disparities = [], []\nfor m in predictors:\n    def classifier(X): return m.predict(X)\n\n    error = ErrorRate()\n    error.load_data(X_train, pd.Series(Y_train), sensitive_features=A_train)\n    disparity = DemographicParity()\n    disparity.load_data(X_train, pd.Series(Y_train), sensitive_features=A_train)\n\n    errors.append(error.gamma(classifier)[0])\n    disparities.append(disparity.gamma(classifier).max())\n\nall_results = pd.DataFrame({\"predictor\": predictors, \"error\": errors, \"disparity\": disparities})\n\nnon_dominated = []\nfor row in all_results.itertuples():\n    errors_for_lower_or_eq_disparity = all_results[\"error\"][all_results[\"disparity\"] <= row.disparity]\n    if row.error <= errors_for_lower_or_eq_disparity.min():\n        non_dominated.append(row.predictor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can put the dominant models into the Fairness dashboard, along with the\nunmitigated model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dashboard_predicted = {\"unmitigated\": unmitigated_predictor.predict(X_test)}\nfor i in range(len(non_dominated)):\n    key = \"dominant_model_{0}\".format(i)\n    value = non_dominated[i].predict(X_test)\n    dashboard_predicted[key] = value\n\n\nFairlearnDashboard(sensitive_features=A_test, sensitive_feature_names=['sex'],\n                   y_true=Y_test,\n                   y_pred=dashboard_predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see a Pareto front forming - the set of predictors which represent optimal tradeoffs\nbetween accuracy and disparity in predictions.\nIn the ideal case, we would have a predictor at (1,0) - perfectly accurate and without\nany unfairness under demographic parity (with respect to the sensitive feature \"sex\").\nThe Pareto front represents the closest we can come to this ideal based on our data and\nchoice of estimator.\nNote the range of the axes - the disparity axis covers more values than the accuracy,\nso we can reduce disparity substantially for a small loss in accuracy.\n\nBy clicking on individual models on the plot, we can inspect their metrics for disparity\nand accuracy in greater detail.\nIn a real example, we would then pick the model which represented the best trade-off\nbetween accuracy and disparity given the relevant business constraints.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}